{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruiqianyang/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "train = pd.read_pickle('train_Basic.pkl')\n",
    "test = pd.read_pickle('test_Basic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "###some FE\n",
    "train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "train['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "test['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "test['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "train['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\n",
    "train['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\n",
    "train['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "test['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\n",
    "test['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\n",
    "test['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n",
    "\n",
    "train['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\n",
    "train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n",
    "train['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\n",
    "train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "test['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\n",
    "test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n",
    "test['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\n",
    "test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "train['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\n",
    "train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n",
    "train['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\n",
    "train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "test['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\n",
    "test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n",
    "test['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\n",
    "test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train[c + '_bin'] = train[c].map(emails)\n",
    "    test[c + '_bin'] = test[c].map(emails)\n",
    "    \n",
    "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "### https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nulls int64\n",
      "month int64\n",
      "hour int64\n",
      "TransactionAmt float64\n",
      "TransactionAmt_decimal int64\n",
      "D5 float64\n",
      "D2 float64\n",
      "D8 float64\n",
      "D9 int64\n",
      "D11 float64\n",
      "D15 float64\n",
      "C1 float64\n",
      "C4 float64\n",
      "C8 float64\n",
      "C10 float64\n",
      "C13 float64\n",
      "V256 float64\n",
      "V13 float64\n",
      "card1 int64\n",
      "card2 float64\n",
      "card3 float64\n",
      "card5 float64\n",
      "addr1 float64\n",
      "DeviceInfo category\n",
      "V145 float64\n",
      "V3 float64\n",
      "V4 float64\n",
      "V5 float64\n",
      "V6 float64\n",
      "V7 float64\n",
      "V8 float64\n",
      "V9 float64\n",
      "V10 float64\n",
      "V11 float64\n",
      "V12 float64\n",
      "V17 float64\n",
      "V19 float64\n",
      "V20 float64\n",
      "V29 float64\n",
      "V30 float64\n",
      "V33 float64\n",
      "V34 float64\n",
      "V35 float64\n",
      "V36 float64\n",
      "V37 float64\n",
      "V38 float64\n",
      "V40 float64\n",
      "V44 float64\n",
      "V45 float64\n",
      "V46 float64\n",
      "V47 float64\n",
      "V48 float64\n",
      "V49 float64\n",
      "V51 float64\n",
      "V52 float64\n",
      "V53 float64\n",
      "V54 float64\n",
      "V56 float64\n",
      "V58 float64\n",
      "V59 float64\n",
      "V60 float64\n",
      "V61 float64\n",
      "V62 float64\n",
      "V63 float64\n",
      "V64 float64\n",
      "V69 float64\n",
      "V70 float64\n",
      "V71 float64\n",
      "V72 float64\n",
      "V73 float64\n",
      "V74 float64\n",
      "V75 float64\n",
      "V76 float64\n",
      "V78 float64\n",
      "V80 float64\n",
      "V81 float64\n",
      "V82 float64\n",
      "V83 float64\n",
      "V84 float64\n",
      "V85 float64\n",
      "V87 float64\n",
      "V90 float64\n",
      "V91 float64\n",
      "V92 float64\n",
      "V93 float64\n",
      "V94 float64\n",
      "V95 float64\n",
      "V96 float64\n",
      "V97 float64\n",
      "V99 float64\n",
      "V100 float64\n",
      "V126 float64\n",
      "V127 float64\n",
      "V128 float64\n",
      "V130 float64\n",
      "V131 float64\n",
      "V138 float64\n",
      "V139 float64\n",
      "V140 float64\n",
      "V143 float64\n",
      "V146 float64\n",
      "V147 float64\n",
      "V149 float64\n",
      "V150 float64\n",
      "V151 float64\n",
      "V152 float64\n",
      "V154 float64\n",
      "V156 float64\n",
      "V158 float64\n",
      "V159 float64\n",
      "V160 float64\n",
      "V161 float64\n",
      "V162 float64\n",
      "V163 float64\n",
      "V164 float64\n",
      "V165 float64\n",
      "V166 float64\n",
      "V167 float64\n",
      "V169 float64\n",
      "V170 float64\n",
      "V171 float64\n",
      "V172 float64\n",
      "V173 float64\n",
      "V175 float64\n",
      "V176 float64\n",
      "V177 float64\n",
      "V178 float64\n",
      "V180 float64\n",
      "V182 float64\n",
      "V184 float64\n",
      "V187 float64\n",
      "V188 float64\n",
      "V189 float64\n",
      "V195 float64\n",
      "V197 float64\n",
      "V200 float64\n",
      "V201 float64\n",
      "V202 float64\n",
      "V203 float64\n",
      "V204 float64\n",
      "V205 float64\n",
      "V206 float64\n",
      "V207 float64\n",
      "V208 float64\n",
      "V209 float64\n",
      "V210 float64\n",
      "V212 float64\n",
      "V213 float64\n",
      "V214 float64\n",
      "V215 float64\n",
      "V216 float64\n",
      "V217 float64\n",
      "V219 float64\n",
      "V220 float64\n",
      "V221 float64\n",
      "V222 float64\n",
      "V223 float64\n",
      "V224 float64\n",
      "V225 float64\n",
      "V226 float64\n",
      "V227 float64\n",
      "V228 float64\n",
      "V229 float64\n",
      "V231 float64\n",
      "V233 float64\n",
      "V234 float64\n",
      "V238 float64\n",
      "V239 float64\n",
      "V242 float64\n",
      "V243 float64\n",
      "V244 float64\n",
      "V245 float64\n",
      "V246 float64\n",
      "V247 float64\n",
      "V249 float64\n",
      "V251 float64\n",
      "V253 float64\n",
      "V257 float64\n",
      "V258 float64\n",
      "V259 float64\n",
      "V261 float64\n",
      "V262 float64\n",
      "V263 float64\n",
      "V264 float64\n",
      "V265 float64\n",
      "V266 float64\n",
      "V267 float64\n",
      "V268 float64\n",
      "V270 float64\n",
      "V271 float64\n",
      "V272 float64\n",
      "V273 float64\n",
      "V274 float64\n",
      "V275 float64\n",
      "V276 float64\n",
      "V277 float64\n",
      "V278 float64\n",
      "V279 float64\n",
      "V280 float64\n",
      "V282 float64\n",
      "V283 float64\n",
      "V285 float64\n",
      "V287 float64\n",
      "V288 float64\n",
      "V289 float64\n",
      "V291 float64\n",
      "V292 float64\n",
      "V294 float64\n",
      "V303 float64\n",
      "V304 float64\n",
      "V306 float64\n",
      "V307 float64\n",
      "V308 float64\n",
      "V310 float64\n",
      "V312 float64\n",
      "V313 float64\n",
      "V314 float64\n",
      "V315 float64\n",
      "V317 float64\n",
      "V322 float64\n",
      "V323 float64\n",
      "V324 float64\n",
      "V326 float64\n",
      "V329 float64\n",
      "V331 float64\n",
      "V332 float64\n",
      "V333 float64\n",
      "V335 float64\n",
      "V336 float64\n",
      "id_01 float64\n",
      "id_03 float64\n",
      "id_05 float64\n",
      "id_06 float64\n",
      "id_09 float64\n",
      "id_11 float64\n",
      "id_13 category\n",
      "id_14 category\n",
      "id_32 category\n",
      "M4_count int64\n",
      "M6_count int64\n",
      "card1_count int64\n",
      "card6_count int64\n",
      "DeviceType_count int64\n",
      "ProductCD_count int64\n",
      "P_emaildomain_count int64\n",
      "addr1_count int64\n",
      "card4_dic float64\n",
      "ProductCD_dic int64\n",
      "card6_dic float64\n",
      "M1_dic float64\n",
      "M4_dic float64\n",
      "M5_dic float64\n",
      "M6_dic float64\n",
      "P_emaildomain_dic float64\n",
      "R_emaildomain_dic float64\n",
      "C2_TransactionDT int64\n",
      "id_30_count int64\n",
      "id_06_count int64\n",
      "TransactionAmt_check int64\n",
      "uid_count int64\n",
      "uid2_count int64\n",
      "uid3_count int64\n",
      "card1_TransactionAmt_mean float64\n",
      "card1_TransactionAmt_std float64\n",
      "card2_TransactionAmt_mean float64\n",
      "card2_TransactionAmt_std float64\n",
      "card3_TransactionAmt_mean float64\n",
      "card3_TransactionAmt_std float64\n",
      "card5_TransactionAmt_mean float64\n",
      "card5_TransactionAmt_std float64\n"
     ]
    }
   ],
   "source": [
    "# for i, j in X.dtypes.reset_index().values:\n",
    "#     print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# for feature in ['id_02__id_20','id_02__D8','D11__DeviceInfo','DeviceInfo__P_emaildomain',\n",
    "#                 'P_emaildomain__C2', 'card2__dist1', 'card1__card5',\n",
    "#                 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1','card2__card2_count']:\n",
    "#'card1__card5','card5__P_emaildomain'\n",
    "\n",
    "#train['card2_count'] =train['card2'].map(pd.concat([train['card2'], test['card2']], ignore_index=True).value_counts(dropna=False))\n",
    "#test['card2_count'] =test['card2'].map(pd.concat([train['card2'], test['card2']], ignore_index=True).value_counts(dropna=False))\n",
    "#'TransactionAmt_decimal__card1','TransactionAmt_decimal__card2',\n",
    "#                'TransactionAmt_decimal__card3','TransactionAmt_decimal__card5',\n",
    "#                 'TransactionAmt_decimal__addr1','card1__hour','card2__hour','card3__hour',\n",
    "#                'card5__hour','addr1__hour','TransactionAmt_decimal__hour',\n",
    "#                 'TransactionAmt_decimal__TransactionDT','C2__TransactionDT'\n",
    "\n",
    "\n",
    "for feature in ['card1__dist1','card1__P_emaildomain','addr1__card1','card4__dist1',\n",
    "                'addr1__card4','addr1__card2','card1__card5','card5__P_emaildomain']:\n",
    "\n",
    "    f1, f2 = feature.split('__')\n",
    "    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n",
    "    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n",
    "\n",
    "#   le = LabelEncoder()\n",
    "#   le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n",
    "#   train[feature] = le.transform(list(train[feature].astype(str).values))\n",
    "#   test[feature] = le.transform(list(test[feature].astype(str).values))\n",
    "    \n",
    " \n",
    "   # train[feature]=train[feature].astype('category')\n",
    "   # test[feature]=test[feature].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruiqianyang/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "y = train['isFraud']\n",
    "X = pd.DataFrame()\n",
    "\n",
    "col=['TransactionDT','ProductCD','P_emaildomain','R_emaildomain','nulls','month','hour',\n",
    "     'TransactionAmt', 'TransactionAmt_decimal','D5','D2','D8','D9','D11','D15','C1','C4','C8','C10','C13','C2',\n",
    "     'V256','V13','card1','card2','card3','card4', 'card5', 'card6','addr1','M4','M5','M6','M1',\n",
    "     'M2','M3','M7','M8','M9','DeviceType',\n",
    "     'DeviceInfo', 'addr2','D6','D13','C5','C9','D7','C14','V145',\n",
    "    'V3','V4','V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V17',\n",
    "    'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n",
    "    'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n",
    "    'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n",
    "    'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n",
    "    'V143', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n",
    "    'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n",
    "    'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n",
    "    'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n",
    "    'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n",
    "    'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V257', 'V258', 'V259', 'V261',\n",
    "    'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n",
    "    'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n",
    "    'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', \n",
    "    'V323','V324', 'V326','V329','V331','V332', 'V333', 'V335', 'V336',\n",
    "    'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n",
    "    'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n",
    "    'id_36', 'id_37', 'id_38',\n",
    "     'isFraud']\n",
    "\n",
    "#'V48','V53','V96'\n",
    "#  'V338'\n",
    "#month is important\n",
    "# Save to pickle \n",
    "New_DatasetName = \"Basic\"\n",
    "#train[col].to_pickle(\"train_{}.pkl\".format(New_DatasetName))\n",
    "#test[col[:-1]].to_pickle(\"test_{}.pkl\".format(New_DatasetName))\n",
    "col.pop()\n",
    "X[col] = train[col]\n",
    "\n",
    "X['M4_count'] =X['M4'].map(pd.concat([train['M4'], test['M4']], ignore_index=True).value_counts(dropna=False))\n",
    "X['M6_count'] =X['M6'].map(pd.concat([train['M6'], test['M6']], ignore_index=True).value_counts(dropna=False))\n",
    "X['card1_count'] = X['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\n",
    "X['card6_count'] =X['card6'].map(pd.concat([train['card6'], test['card6']], ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "X['DeviceType_count'] =X['DeviceType'].map(pd.concat([train['DeviceType'], test['DeviceType']], ignore_index=True).value_counts(dropna=False))\n",
    "#X['DeviceInfo_count'] =X['DeviceInfo'].map(pd.concat([train['DeviceInfo'], test['DeviceInfo']], ignore_index=True).value_counts(dropna=False))\n",
    "X['ProductCD_count'] =X['ProductCD'].map(pd.concat([train['ProductCD'], test['ProductCD']], ignore_index=True).value_counts(dropna=False))\n",
    "X['P_emaildomain_count'] =X['P_emaildomain'].map(pd.concat([train['P_emaildomain'], test['P_emaildomain']], ignore_index=True).value_counts(dropna=False))\n",
    "#X['R_emaildomain_count'] =X['R_emaildomain'].map(pd.concat([train['R_emaildomain'], test['R_emaildomain']], ignore_index=True).value_counts(dropna=False))\n",
    "X['addr1_count'] = X['addr1'].map(pd.concat([train['addr1'], test['addr1']], ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "# Risk mapping transformation\n",
    "card4_dic = {'american express':287,'discover':773,'mastercard':343,'visa':348}\n",
    "X['card4_dic']=X['card4'].map(card4_dic)\n",
    "#train=train.replace({'card4':card4_dic})\n",
    "#test=test.replace({'card4':card4_dic})\n",
    "ProductCD_dic = {'C':117,'H':48,'R':38,'S':59,'W':20}\n",
    "X['ProductCD_dic']=X['ProductCD'].map(ProductCD_dic)\n",
    "\n",
    "card6_dic = {'charge card':0,'credit':668,'debit':243,'debit or credit':0}\n",
    "X['card6_dic']=X['card6'].map(card6_dic)\n",
    "#train=train.replace({'card6':card6_dic})\n",
    "#test=test.replace({'card6':card6_dic})\n",
    "\n",
    "M1_dic={'F':0,'T':2}\n",
    "X['M1_dic']=X['M1'].map(M1_dic)\n",
    "\n",
    "M4_dic={'M0':4,'M1':3,'M2':13}\n",
    "X['M4_dic']=X['M4'].map(M4_dic)\n",
    "\n",
    "M5_dic={'F':2,'T':3}\n",
    "X['M5_dic']=X['M5'].map(M5_dic)\n",
    "\n",
    "M6_dic={'F':4,'T':3}\n",
    "X['M6_dic']=X['M6'].map(M6_dic)\n",
    "\n",
    "#DeviceInfo has many category levels\n",
    "#DeviceType_dic={'desktop':3,'mobile':5}\n",
    "#X['DeviceType_dic']=X['DeviceType'].map(DeviceType_dic)\n",
    "\n",
    "P_emaildomain_dic={'protonmail.com':40,'mail.com':19,'outlook.es':13,'aim.com':12,\n",
    "                    'outlook.com':9,'hotmail.es':7,'live.com.mx':5,'hotmail.com':5,'gmail.com':4}\n",
    "X['P_emaildomain_dic']=X['P_emaildomain'].map(P_emaildomain_dic)\n",
    "\n",
    "\n",
    "R_emaildomain_dic={'protonmail.com':95,'mail.com':38,'netzero.net':22,'outlook.com':17,\n",
    "                    'outlook.es':13,'icloud.com':13,'gmail.com':12,'hotmail.com':8,\n",
    "                    'earthlink.net':8,'earthlink.net':7,'hotmail.es':7,'live.com.mx':6,\n",
    "                   'yahoo.com':5,'live.com':5}\n",
    "X['R_emaildomain_dic']=X['R_emaildomain'].map(R_emaildomain_dic)\n",
    "\n",
    "#New feature:C2 vs TransactionDT\n",
    "def func_C2_Tran(a,b):\n",
    "        if a<400:\n",
    "            return 344\n",
    "        elif a>=400 and a<=651 and b<=4000000:\n",
    "            return 3846\n",
    "        elif a>=400 and a<=651 and b>10000000:\n",
    "            return 10000\n",
    "        else:\n",
    "            return 1082\n",
    "X['C2_TransactionDT']=X.apply(lambda x:func_C2_Tran(x['C2'],x['TransactionDT']),axis=1)\n",
    "\n",
    "for feature in ['id_30','id_06']:\n",
    "        # Count encoded separately for train and test,\n",
    "    X[feature + '_count'] = X[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "\n",
    "###sep 06,2019 add fill na:\n",
    "X['D9'] = np.where(X['D9'].isna(),0,1)\n",
    "\n",
    "########################### Reset values for \"noise\" card1\n",
    "# i_cols = ['card1']\n",
    "\n",
    "# for col in i_cols: \n",
    "#     valid_card = pd.concat([train[[col]], test[[col]]])\n",
    "#     valid_card = valid_card[col].value_counts()\n",
    "#     valid_card = valid_card[valid_card>2]\n",
    "#     valid_card = list(valid_card.index)\n",
    "\n",
    "#     X[col] = np.where(train[col].isin(test[col]), train[col], np.nan)\n",
    "#     X[col] = np.where(X[col].isin(valid_card), X[col], np.nan)\n",
    "# #     test_x[col]  = np.where(test_x[col].isin(train[col]), test_x[col], np.nan)\n",
    "# #     test_x[col]  = np.where(test_x[col].isin(valid_card), test_x[col], np.nan)\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "# Check if the Transaction Amount is common or not (we can use freq encoding here)\n",
    "# In our dialog with a model we are telling to trust or not to these values   \n",
    "X['TransactionAmt_check'] = np.where(X['TransactionAmt'].isin(test['TransactionAmt']), 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########Interaction term\n",
    "train['uid'] = train['card1'].astype(str)+'_'+train['card2'].astype(str)\n",
    "test['uid'] = test['card1'].astype(str)+'_'+test['card2'].astype(str)\n",
    "\n",
    "train['uid2'] = train['uid'].astype(str)+'_'+train['card3'].astype(str)+'_'+train['card5'].astype(str)\n",
    "test['uid2'] = test['uid'].astype(str)+'_'+test['card3'].astype(str)+'_'+test['card5'].astype(str)\n",
    "\n",
    "train['uid3'] = train['uid2'].astype(str)+'_'+train['addr1'].astype(str)+'_'+train['addr2'].astype(str)\n",
    "test['uid3'] = test['uid2'].astype(str)+'_'+test['addr1'].astype(str)+'_'+test['addr2'].astype(str)\n",
    "\n",
    "X['uid'] = train['uid']\n",
    "X['uid2'] = train['uid2']\n",
    "X['uid3'] = train['uid3']\n",
    "\n",
    "    ###########\n",
    "for feature in ['uid','uid2','uid3']:\n",
    "        # Count encoded separately for train and test,\n",
    "    X[feature + '_count'] = X[feature].map(pd.concat([train[feature], test[feature]], \n",
    "                                                     ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "# For our model current TransactionAmt is a noise\n",
    "# https://www.kaggle.com/kyakovlev/ieee-check-noise\n",
    "# (even if features importances are telling contrariwise)\n",
    "# There are many unique values and model doesn't generalize well\n",
    "# Lets do some aggregations\n",
    "\n",
    "i_cols = ['card1','card2','card3','card5']\n",
    "#'uid','uid2','uid3'\n",
    "for col2 in i_cols:\n",
    "    for agg_type in ['mean','std']:\n",
    "        new_col_name = col2+'_TransactionAmt_'+agg_type\n",
    "        temp_df = pd.concat([train[[col2, 'TransactionAmt']], test[[col2,'TransactionAmt']]])\n",
    "        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "        temp_df = temp_df.groupby([col2])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "                                                columns={agg_type: new_col_name})\n",
    "        \n",
    "        temp_df.index = list(temp_df[col2])\n",
    "        temp_df = temp_df[new_col_name].to_dict()   \n",
    "    \n",
    "        X[new_col_name] = X[col2].map(temp_df)\n",
    "        \n",
    "\n",
    "# Small \"hack\" to transform distribution \n",
    "# (doesn't affect auc much, but I like it more)\n",
    "# please see how distribution transformation can boost your score \n",
    "# (not our case but related)\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html\n",
    "\n",
    "#X['TransactionAmt'] = np.log1p(X['TransactionAmt'])\n",
    "\n",
    "for df in [train,test,X]:\n",
    "    ########################### Device info\n",
    "    if df['DeviceInfo'].isna().sum() !=0: \n",
    "        df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "#    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "#    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "#     ########################### Device info 2\n",
    "#    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n",
    "#    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "#    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "    \n",
    "#     ########################### Browser\n",
    "#    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n",
    "#     df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "\n",
    "    \n",
    "# ########label encoding category feature device\n",
    "for col_le in ['DeviceInfo']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[col_le])+list(test[col_le]))  \n",
    "    X[col_le]=le.transform(X[col_le])\n",
    "    X[col_le]=X[col_le].astype('category')   \n",
    "    \n",
    "    \n",
    "X=X.drop(columns=['ProductCD','P_emaildomain','R_emaildomain','M4','M5','M6','M1', 'M2','M3','M7','M8','M9','card6', 'DeviceType',\n",
    "                  'card4','id_02','id_19','id_20','id_17','C2','TransactionDT','addr2','D6','D13','C5','C9','D7','C14',\n",
    "                  'id_12', 'id_15','id_30','id_31', 'id_33', 'id_36', 'id_37', 'id_38','uid','uid2','uid3'\n",
    "                 ])\n",
    "\n",
    "# for col in X.columns:\n",
    "#     if X[col].dtype == 'object':\n",
    "#         print(col)\n",
    "#         le = LabelEncoder()\n",
    "#         le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n",
    "#         X[col] = le.transform(list(X[col].astype(str).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['addr1'].fillna(0, inplace=True)  \n",
    "###X['addr1'].fillna(X['addr1'].mean(), inplace=True)  \n",
    "#X['addr1']=X['addr1'].cat.add_categories(0).fillna(0)#fill as category\n",
    "\n",
    "#X['addr2'].fillna(X['addr2'].mean(), inplace=True)\n",
    "#X['card2']=train['card2']\n",
    "X['card2'].fillna(502.0, inplace=True)  #fill as category?\n",
    "#X['card2'].fillna(X['card2'].mode()[0], inplace=True) \n",
    "\n",
    "#X['card5']=train['card5']\n",
    "###X['card5'].fillna(228.0, inplace=True)\n",
    "\n",
    "\n",
    "X['D2'].fillna(10, inplace=True)\n",
    "X['D11'].fillna(10, inplace=True)\n",
    "X['D15'].fillna(376, inplace=True)\n",
    "#X['D5'].fillna(0, inplace=True)\n",
    "###X['D8'].fillna(X['D8'].mean(), inplace=True)\n",
    "#X['V1'].fillna(X['V1'].mode()[0], inplace=True)          \n",
    "#X['V2'].fillna(3, inplace=True)\n",
    "X['V13'].fillna(X['V13'].mode()[0], inplace=True)\n",
    "#fillna reduce score\n",
    "#X['V263'].fillna(X['V263'].mode()[0], inplace=True)\n",
    "#X['V256'].fillna(X['V256'].mode()[0], inplace=True)\n",
    "\n",
    "X['card4_dic'].fillna(X['card4_dic'].mode()[0], inplace=True)\n",
    "#X['card6_dic'].fillna(X['card6_dic'].mean(), inplace=True)\n",
    "                   \n",
    "X['M4_dic'].fillna(2, inplace=True)\n",
    "X['M5_dic'].fillna(3, inplace=True)  #24:18:78\n",
    "X['M6_dic'].fillna(13, inplace=True)  #24:18:78\n",
    "\n",
    "#X['cross'].fillna(2.6, inplace=True)\n",
    "#X['id_34'].fillna(2, inplace=True)\n",
    "#X['id_19'].fillna(444, inplace=True)\n",
    "#X['id_20'].fillna(266, inplace=True)\n",
    "#X['id_17'].fillna(133, inplace=True)\n",
    "X['P_emaildomain_dic'].fillna(0, inplace=True)\n",
    "X['R_emaildomain_dic'].fillna(0, inplace=True)\n",
    "\n",
    "#sep 04,2019 add fill na:\n",
    "#X['V145'].fillna(X['V145'].mean(), inplace=True)  #V145 has more than 80% missing\n",
    "X['V19'].fillna(0, inplace=True)\n",
    "X['V36'].fillna(0, inplace=True)\n",
    "X['V64'].fillna(0, inplace=True)\n",
    "X['V70'].fillna(0, inplace=True)\n",
    "X['V80'].fillna(0, inplace=True)\n",
    "X['V94'].fillna(0, inplace=True)\n",
    "# X['V143'].fillna(X['V143'].mean(), inplace=True)\n",
    "# X['V150'].fillna(X['V150'].mean(), inplace=True)\n",
    "# X['V152'].fillna(X['V152'].mean(), inplace=True)\n",
    "X['V158'].fillna(0, inplace=True)##\n",
    "# X['V163'].fillna(X['V163'].mean(), inplace=True)\n",
    "# X['V165'].fillna(X['V165'].mean(), inplace=True)\n",
    "# X['V177'].fillna(X['V177'].mean(), inplace=True)\n",
    "# X['V204'].fillna(X['V204'].mean(), inplace=True)\n",
    "# X['V207'].fillna(X['V207'].mean(), inplace=True)\n",
    "# X['V209'].fillna(X['V209'].mean(), inplace=True) ##\n",
    "# X['V221'].fillna(X['V221'].mean(), inplace=True)\n",
    "# X['V222'].fillna(X['V222'].mean(), inplace=True)\n",
    "# X['V266'].fillna(X['V266'].mean(), inplace=True)\n",
    "# X['V267'].fillna(X['V267'].mean(), inplace=True)\n",
    "# X['V274'].fillna(X['V274'].mean(), inplace=True)\n",
    "# X['V275'].fillna(X['V275'].mean(), inplace=True)\n",
    "X['V279'].fillna(0, inplace=True)\n",
    "X['V283'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "dates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\n",
    "us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n",
    "\n",
    "#X['DT_W'] = ((train['TransactionDT1'].dt.year-2017)*52 + train['TransactionDT1'].dt.weekofyear).astype(np.int8)\n",
    "#X['DT_D'] = ((train['TransactionDT1'].dt.year-2017)*365 + train['TransactionDT1'].dt.dayofyear).astype(np.int16)   \n",
    "\n",
    "X['DT_M'] = ((train['TransactionDT1'].dt.year-2017)*12 + train['TransactionDT1'].dt.month).astype(np.int8)\n",
    "\n",
    "X['is_december'] = (train['month']==12).astype(np.int8)\n",
    "\n",
    "# Holidays\n",
    "X['is_holiday'] = (train['TransactionDT1'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nulls','month','hour','TransactionAmt','TransactionAmt_decimal','D5','D2','D8','D9','D11','D15','C1','C4','C8','C10','C13','V256','V13','card1','card2','card3','card5','addr1','DeviceInfo','V145','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V17','V19','V20','V29','V30','V33','V34','V35','V36','V37','V38','V40','V44','V45','V46','V47','V48','V49','V51','V52','V53','V54','V56','V58','V59','V60','V61','V62','V63','V64','V69','V70','V71','V72','V73','V74','V75','V76','V78','V80','V81','V82','V83','V84','V85','V87','V90','V91','V92','V93','V94','V95','V96','V97','V99','V100','V126','V127','V128','V130','V131','V138','V139','V140','V143','V146','V147','V149','V150','V151','V152','V154','V156','V158','V159','V160','V161','V162','V163','V164','V165','V166','V167','V169','V170','V171','V172','V173','V175','V176','V177','V178','V180','V182','V184','V187','V188','V189','V195','V197','V200','V201','V202','V203','V204','V205','V206','V207','V208','V209','V210','V212','V213','V214','V215','V216','V217','V219','V220','V221','V222','V223','V224','V225','V226','V227','V228','V229','V231','V233','V234','V238','V239','V242','V243','V244','V245','V246','V247','V249','V251','V253','V257','V258','V259','V261','V262','V263','V264','V265','V266','V267','V268','V270','V271','V272','V273','V274','V275','V276','V277','V278','V279','V280','V282','V283','V285','V287','V288','V289','V291','V292','V294','V303','V304','V306','V307','V308','V310','V312','V313','V314','V315','V317','V322','V323','V324','V326','V329','V331','V332','V333','V335','V336','id_01','id_03','id_05','id_06','id_09','id_11','id_13','id_14','id_32','M4_count','M6_count','card1_count','card6_count','DeviceType_count','ProductCD_count','P_emaildomain_count','addr1_count','card4_dic','ProductCD_dic','card6_dic','M1_dic','M4_dic','M5_dic','M6_dic','P_emaildomain_dic','R_emaildomain_dic','C2_TransactionDT','id_30_count','id_06_count','TransactionAmt_check','uid_count','uid2_count','uid3_count','card1_TransactionAmt_mean','card1_TransactionAmt_std','card2_TransactionAmt_mean','card2_TransactionAmt_std','card3_TransactionAmt_mean','card3_TransactionAmt_std','card5_TransactionAmt_mean','card5_TransactionAmt_std',"
     ]
    },
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in list(X):\n",
    "    print(\"'\"+str(i)+\"'\",end=',')\n",
    "len(list(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                             y, test_size=0.33, random_state=47, shuffle=False)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X.drop(columns=['DT_M']), \n",
    "#                              y, test_size=0.2, random_state=47, stratify =y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9212473430497328"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'boosting_type':'gbdt', 'class_weight':None, 'colsample_bytree':0.7,\n",
    "        'importance_type':'split', 'learning_rate':0.05, 'max_depth':25,\n",
    "        'min_child_samples':20, 'min_child_weight':0.00298, 'min_split_gain':0.0,\n",
    "        'n_estimators':300, 'n_jobs':-1, 'num_leaves':256, 'silent':False, 'subsample':0.7,\n",
    "          'reg_alpha':0.38999, 'reg_lambda':2,'subsample_for_bin':200000,'subsample_freq':1,\n",
    "        'objective': 'binary', \"bagging_seed\": 8, 'metric': 'auc', 'random_state': 47}             \n",
    "clf = lgb.LGBMClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9705351546704877"
      ]
     },
     "execution_count": 1654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stratify=y\n",
    "roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################Interaction term for test data\n",
    "test_x['uid'] = test['uid']\n",
    "test_x['uid2'] = test['uid2']\n",
    "test_x['uid3'] = test['uid3']\n",
    "\n",
    "###########\n",
    "for feature in ['uid','uid2','uid3']:\n",
    "        # Count encoded separately for train and test,\n",
    "    test_x[feature + '_count'] = test_x[feature].map(pd.concat([train[feature], test[feature]], \n",
    "                                                               ignore_index=True).value_counts(dropna=False))\n",
    "\n",
    "# Check if the Transaction Amount is common or not (we can use freq encoding here)\n",
    "# In our dialog with a model we are telling to trust or not to these values   \n",
    "\n",
    "test_x['TransactionAmt_check']  = np.where(test_x['TransactionAmt'].isin(train['TransactionAmt']), 1, 0)\n",
    "\n",
    "i_cols = ['card1','card2','card3','card5']\n",
    "\n",
    "for col2 in i_cols:\n",
    "    for agg_type in ['mean','std']:\n",
    "        new_col_name = col2+'_TransactionAmt_'+agg_type\n",
    "        temp_df = pd.concat([train[[col2, 'TransactionAmt']], test[[col2,'TransactionAmt']]])\n",
    "        #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "        temp_df = temp_df.groupby([col2])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "                                                columns={agg_type: new_col_name})\n",
    "        \n",
    "        temp_df.index = list(temp_df[col2])\n",
    "        temp_df = temp_df[new_col_name].to_dict()   \n",
    "    \n",
    "        test_x[new_col_name]  = test_x[col2].map(temp_df)\n",
    "        \n",
    "# i_cols2 = ['uid','uid2','uid3']\n",
    "\n",
    "# for col3 in i_cols2:\n",
    "#     for agg_type in ['mean','std']:\n",
    "#         new_col_name = col3+'_TransactionAmt_'+agg_type\n",
    "#         temp_df = pd.concat([X[[col3, 'TransactionAmt']], test[[col3,'TransactionAmt']]])\n",
    "#         #temp_df['TransactionAmt'] = temp_df['TransactionAmt'].astype(int)\n",
    "#         temp_df = temp_df.groupby([col3])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n",
    "#                                                 columns={agg_type: new_col_name})\n",
    "        \n",
    "#         temp_df.index = list(temp_df[col3])\n",
    "#         temp_df = temp_df[new_col_name].to_dict()   \n",
    "    \n",
    "#         test_x[new_col_name] = test_x[col3].map(temp_df) \n",
    "                   \n",
    "\n",
    "# Small \"hack\" to transform distribution \n",
    "# (doesn't affect auc much, but I like it more)\n",
    "# please see how distribution transformation can boost your score \n",
    "# (not our case but related)\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_transformed_target.html\n",
    "\n",
    "#test_x['TransactionAmt'] = np.log1p(test_x['TransactionAmt']) \n",
    "\n",
    "########################### Device info\n",
    "test_x['DeviceInfo'] = test_x['DeviceInfo'].fillna('unknown_device').str.lower()\n",
    "# test_x['DeviceInfo_device'] = test_x['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "# test_x['DeviceInfo_version'] = test_x['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "\n",
    "# ########################### Device info 2\n",
    "# test_x['id_30'] = test_x['id_30'].fillna('unknown_device').str.lower()\n",
    "# test_x['id_30_device'] = test_x['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "# test_x['id_30_version'] = test_x['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n",
    "\n",
    "# ########################### Browser\n",
    "# test_x['id_31'] = test_x['id_31'].fillna('unknown_device').str.lower()\n",
    "# test_x['id_31_device'] = test_x['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n",
    "\n",
    "########label encoding category feature device\n",
    "for col_le in ['DeviceInfo']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(train[col_le])+list(test[col_le]))  \n",
    "    test_x[col_le]=le.transform(test_x[col_le])\n",
    "    test_x[col_le]=test_x[col_le].astype('category')\n",
    "    \n",
    "test_x=test_x.drop(columns=['ProductCD','P_emaildomain','R_emaildomain','M4','M5','M6','M1', 'M2','M3','M7','M8','M9','card6', 'DeviceType',\n",
    "                  'card4','id_02','id_19','id_20','id_17','C2','TransactionDT','addr2','D6','D13','C5','C9','D7','C14',\n",
    "                  'id_12', 'id_15', 'id_30', 'id_31', 'id_33', 'id_36', 'id_37', 'id_38','uid','uid2','uid3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing in test data\n",
    "test_x['addr1'].fillna(0, inplace=True)\n",
    "##test_x['D5'].fillna(150, inplace=True)\n",
    "test_x['D2'].fillna(10, inplace=True)\n",
    "test_x['D11'].fillna(10, inplace=True)\n",
    "test_x['D15'].fillna(376, inplace=True)\n",
    "#test_x['V1'].fillna(test_x['V1'].mode()[0], inplace=True)\n",
    "#test_x['V2'].fillna(3, inplace=True)\n",
    "test_x['V13'].fillna(test_x['V13'].mode()[0], inplace=True)\n",
    "##test_x['V256'].fillna(test_x['V256'].mode()[0], inplace=True)\n",
    "test_x['card2'].fillna(502, inplace=True)\n",
    "test_x['card4_dic'].fillna(test_x['card4_dic'].mode()[0], inplace=True)\n",
    "\n",
    "test_x['M4_dic'].fillna(2, inplace=True)\n",
    "test_x['M5_dic'].fillna(3, inplace=True)  #24:18:78\n",
    "test_x['M6_dic'].fillna(13, inplace=True)  #24:18:78\n",
    "test_x['P_emaildomain_dic'].fillna(0, inplace=True)\n",
    "test_x['R_emaildomain_dic'].fillna(0, inplace=True)\n",
    "\n",
    "# test_x['V145'].fillna(0, inplace=True)\n",
    "test_x['V19'].fillna(0, inplace=True)\n",
    "test_x['V36'].fillna(0, inplace=True)\n",
    "test_x['V64'].fillna(0, inplace=True)\n",
    "test_x['V70'].fillna(0, inplace=True)\n",
    "test_x['V80'].fillna(0, inplace=True)\n",
    "test_x['V94'].fillna(0, inplace=True)\n",
    "# test_x['V143'].fillna(0, inplace=True)\n",
    "# test_x['V150'].fillna(0, inplace=True)\n",
    "# test_x['V152'].fillna(0, inplace=True)\n",
    "test_x['V158'].fillna(0, inplace=True)##\n",
    "# test_x['V163'].fillna(0, inplace=True)\n",
    "# test_x['V165'].fillna(0, inplace=True)\n",
    "# test_x['V177'].fillna(0, inplace=True)\n",
    "# test_x['V204'].fillna(0, inplace=True)\n",
    "# test_x['V207'].fillna(0, inplace=True)\n",
    "# test_x['V209'].fillna(0, inplace=True) ##\n",
    "# test_x['V221'].fillna(0, inplace=True)\n",
    "# test_x['V222'].fillna(0, inplace=True)\n",
    "# test_x['V266'].fillna(0, inplace=True)\n",
    "# test_x['V267'].fillna(0, inplace=True)\n",
    "# test_x['V274'].fillna(0, inplace=True)\n",
    "# test_x['V275'].fillna(0, inplace=True)\n",
    "test_x['V279'].fillna(0, inplace=True)\n",
    "test_x['V283'].fillna(0, inplace=True)\n",
    "#test_x['id_30_count'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_seed=8, boosting_type='gbdt', class_weight=None,\n",
       "        colsample_bytree=1.0, importance_type='split', learning_rate=0.05,\n",
       "        max_depth=20, metric='auc', min_child_samples=20,\n",
       "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=300,\n",
       "        n_jobs=-1, num_leaves=300, objective='binary', random_state=47,\n",
       "        reg_alpha=0.0, reg_lambda=0.0, silent=False, subsample=1,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if use SMOTE, dateframe will change to np ndarray..\n",
    "# X=np.delete(X, 0, axis=1)\n",
    "# clf.fit(X, y)\n",
    "clf.fit(X, y)\n",
    "params['learning_rate'] = 0.005\n",
    "params['n_estimators'] = 1800\n",
    "params['early_stopping_rounds'] = 100    \n",
    "test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, params, NFOLDS=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayes opt\n",
    "X_bay=X.reset_index(drop=True)\n",
    "y_bay=y.reset_index(drop=True)\n",
    "bayesian_tr_idx, bayesian_val_idx = train_test_split(X_bay, test_size = 0.3, random_state = 42,stratify =y_bay) \n",
    "#                                                      ,stratify = train_df[target])\n",
    "bayesian_tr_idx = bayesian_tr_idx.index\n",
    "bayesian_val_idx = bayesian_val_idx.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bagging_fraction', 'feature_fraction', 'max_depth', 'min_child_weight', 'min_data_in_leaf', 'num_leaves', 'reg_alpha', 'reg_lambda']\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | baggin... | featur... | max_depth | min_ch... | min_da... | num_le... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9429  \u001b[0m | \u001b[0m 0.5247  \u001b[0m | \u001b[0m 0.8704  \u001b[0m | \u001b[0m 26.39   \u001b[0m | \u001b[0m 0.005991\u001b[0m | \u001b[0m 24.68   \u001b[0m | \u001b[0m 119.8   \u001b[0m | \u001b[0m 0.05808 \u001b[0m | \u001b[0m 2.165   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.9493  \u001b[0m | \u001b[95m 0.6607  \u001b[0m | \u001b[95m 0.7248  \u001b[0m | \u001b[95m 22.12   \u001b[0m | \u001b[95m 0.009699\u001b[0m | \u001b[95m 44.97   \u001b[0m | \u001b[95m 151.8   \u001b[0m | \u001b[95m 0.1818  \u001b[0m | \u001b[95m 0.4585  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.9436  \u001b[0m | \u001b[0m 0.4825  \u001b[0m | \u001b[0m 0.6149  \u001b[0m | \u001b[0m 24.59   \u001b[0m | \u001b[0m 0.002919\u001b[0m | \u001b[0m 38.36   \u001b[0m | \u001b[0m 110.4   \u001b[0m | \u001b[0m 0.2921  \u001b[0m | \u001b[0m 0.9159  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.928   \u001b[0m | \u001b[0m 0.5736  \u001b[0m | \u001b[0m 0.7711  \u001b[0m | \u001b[0m 23.2    \u001b[0m | \u001b[0m 0.005147\u001b[0m | \u001b[0m 37.77   \u001b[0m | \u001b[0m 57.43   \u001b[0m | \u001b[0m 0.6075  \u001b[0m | \u001b[0m 0.4263  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9384  \u001b[0m | \u001b[0m 0.339   \u001b[0m | \u001b[0m 0.8693  \u001b[0m | \u001b[0m 27.79   \u001b[0m | \u001b[0m 0.008086\u001b[0m | \u001b[0m 29.14   \u001b[0m | \u001b[0m 86.58   \u001b[0m | \u001b[0m 0.6842  \u001b[0m | \u001b[0m 1.1     \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9604  \u001b[0m | \u001b[95m 0.3732  \u001b[0m | \u001b[95m 0.5971  \u001b[0m | \u001b[95m 22.21   \u001b[0m | \u001b[95m 0.009094\u001b[0m | \u001b[95m 27.76   \u001b[0m | \u001b[95m 408.0   \u001b[0m | \u001b[95m 0.3117  \u001b[0m | \u001b[95m 1.3     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.959   \u001b[0m | \u001b[0m 0.628   \u001b[0m | \u001b[0m 0.4109  \u001b[0m | \u001b[0m 27.82   \u001b[0m | \u001b[0m 0.007754\u001b[0m | \u001b[0m 48.18   \u001b[0m | \u001b[0m 540.2   \u001b[0m | \u001b[0m 0.5979  \u001b[0m | \u001b[0m 2.305   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9481  \u001b[0m | \u001b[0m 0.3531  \u001b[0m | \u001b[0m 0.4176  \u001b[0m | \u001b[0m 22.27   \u001b[0m | \u001b[0m 0.00326 \u001b[0m | \u001b[0m 31.66   \u001b[0m | \u001b[0m 185.4   \u001b[0m | \u001b[0m 0.8287  \u001b[0m | \u001b[0m 0.8919  \u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.9644  \u001b[0m | \u001b[95m 0.4686  \u001b[0m | \u001b[95m 0.6256  \u001b[0m | \u001b[95m 22.85   \u001b[0m | \u001b[95m 0.008024\u001b[0m | \u001b[95m 22.24   \u001b[0m | \u001b[95m 592.5   \u001b[0m | \u001b[95m 0.7722  \u001b[0m | \u001b[95m 0.4968  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.9354  \u001b[0m | \u001b[0m 0.3033  \u001b[0m | \u001b[0m 0.7893  \u001b[0m | \u001b[0m 26.24   \u001b[0m | \u001b[0m 0.007293\u001b[0m | \u001b[0m 43.14   \u001b[0m | \u001b[0m 73.13   \u001b[0m | \u001b[0m 0.3585  \u001b[0m | \u001b[0m 0.2897  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9624  \u001b[0m | \u001b[0m 0.4912  \u001b[0m | \u001b[0m 0.436   \u001b[0m | \u001b[0m 27.79   \u001b[0m | \u001b[0m 0.004332\u001b[0m | \u001b[0m 20.05   \u001b[0m | \u001b[0m 507.7   \u001b[0m | \u001b[0m 0.5997  \u001b[0m | \u001b[0m 0.4214  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9625  \u001b[0m | \u001b[0m 0.8779  \u001b[0m | \u001b[0m 0.716   \u001b[0m | \u001b[0m 27.61   \u001b[0m | \u001b[0m 0.005862\u001b[0m | \u001b[0m 20.4    \u001b[0m | \u001b[0m 599.2   \u001b[0m | \u001b[0m 0.5637  \u001b[0m | \u001b[0m 2.23    \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9618  \u001b[0m | \u001b[0m 0.6442  \u001b[0m | \u001b[0m 0.8949  \u001b[0m | \u001b[0m 27.61   \u001b[0m | \u001b[0m 0.009322\u001b[0m | \u001b[0m 49.82   \u001b[0m | \u001b[0m 341.9   \u001b[0m | \u001b[0m 0.235   \u001b[0m | \u001b[0m 0.1344  \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.9656  \u001b[0m | \u001b[95m 0.5279  \u001b[0m | \u001b[95m 0.8408  \u001b[0m | \u001b[95m 24.16   \u001b[0m | \u001b[95m 0.002303\u001b[0m | \u001b[95m 49.42   \u001b[0m | \u001b[95m 599.0   \u001b[0m | \u001b[95m 0.2128  \u001b[0m | \u001b[95m 0.1568  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.9653  \u001b[0m | \u001b[0m 0.8259  \u001b[0m | \u001b[0m 0.6294  \u001b[0m | \u001b[0m 27.12   \u001b[0m | \u001b[0m 0.008602\u001b[0m | \u001b[0m 26.57   \u001b[0m | \u001b[0m 597.8   \u001b[0m | \u001b[0m 0.09512 \u001b[0m | \u001b[0m 0.001745\u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.9634  \u001b[0m | \u001b[0m 0.3369  \u001b[0m | \u001b[0m 0.3719  \u001b[0m | \u001b[0m 27.5    \u001b[0m | \u001b[0m 0.001126\u001b[0m | \u001b[0m 49.57   \u001b[0m | \u001b[0m 599.5   \u001b[0m | \u001b[0m 0.07047 \u001b[0m | \u001b[0m 0.2164  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.9651  \u001b[0m | \u001b[0m 0.8881  \u001b[0m | \u001b[0m 0.5898  \u001b[0m | \u001b[0m 23.11   \u001b[0m | \u001b[0m 0.003955\u001b[0m | \u001b[0m 20.16   \u001b[0m | \u001b[0m 599.5   \u001b[0m | \u001b[0m 0.0813  \u001b[0m | \u001b[0m 0.04392 \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.9626  \u001b[0m | \u001b[0m 0.7011  \u001b[0m | \u001b[0m 0.8812  \u001b[0m | \u001b[0m 27.74   \u001b[0m | \u001b[0m 0.007304\u001b[0m | \u001b[0m 48.38   \u001b[0m | \u001b[0m 440.5   \u001b[0m | \u001b[0m 0.7693  \u001b[0m | \u001b[0m 0.03221 \u001b[0m |\n",
      "| \u001b[95m 19      \u001b[0m | \u001b[95m 0.9664  \u001b[0m | \u001b[95m 0.82    \u001b[0m | \u001b[95m 0.6522  \u001b[0m | \u001b[95m 22.71   \u001b[0m | \u001b[95m 0.008483\u001b[0m | \u001b[95m 21.69   \u001b[0m | \u001b[95m 598.7   \u001b[0m | \u001b[95m 0.1638  \u001b[0m | \u001b[95m 0.104   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.9659  \u001b[0m | \u001b[0m 0.4113  \u001b[0m | \u001b[0m 0.813   \u001b[0m | \u001b[0m 22.23   \u001b[0m | \u001b[0m 0.006277\u001b[0m | \u001b[0m 22.26   \u001b[0m | \u001b[0m 599.9   \u001b[0m | \u001b[0m 0.1181  \u001b[0m | \u001b[0m 0.06507 \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.9645  \u001b[0m | \u001b[0m 0.7361  \u001b[0m | \u001b[0m 0.6557  \u001b[0m | \u001b[0m 22.06   \u001b[0m | \u001b[0m 0.003892\u001b[0m | \u001b[0m 48.25   \u001b[0m | \u001b[0m 599.0   \u001b[0m | \u001b[0m 0.6171  \u001b[0m | \u001b[0m 0.09919 \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.9642  \u001b[0m | \u001b[0m 0.8765  \u001b[0m | \u001b[0m 0.8639  \u001b[0m | \u001b[0m 26.15   \u001b[0m | \u001b[0m 0.009726\u001b[0m | \u001b[0m 22.46   \u001b[0m | \u001b[0m 598.6   \u001b[0m | \u001b[0m 0.8666  \u001b[0m | \u001b[0m 0.006978\u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.9636  \u001b[0m | \u001b[0m 0.7106  \u001b[0m | \u001b[0m 0.4137  \u001b[0m | \u001b[0m 22.06   \u001b[0m | \u001b[0m 0.0017  \u001b[0m | \u001b[0m 21.62   \u001b[0m | \u001b[0m 594.9   \u001b[0m | \u001b[0m 0.1378  \u001b[0m | \u001b[0m 0.01645 \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.9658  \u001b[0m | \u001b[0m 0.4962  \u001b[0m | \u001b[0m 0.8572  \u001b[0m | \u001b[0m 22.16   \u001b[0m | \u001b[0m 0.008251\u001b[0m | \u001b[0m 20.08   \u001b[0m | \u001b[0m 599.5   \u001b[0m | \u001b[0m 0.2864  \u001b[0m | \u001b[0m 0.2851  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.9653  \u001b[0m | \u001b[0m 0.4389  \u001b[0m | \u001b[0m 0.6619  \u001b[0m | \u001b[0m 22.44   \u001b[0m | \u001b[0m 0.002942\u001b[0m | \u001b[0m 20.73   \u001b[0m | \u001b[0m 598.2   \u001b[0m | \u001b[0m 0.01266 \u001b[0m | \u001b[0m 0.08518 \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#black box LGBM \n",
    "from bayes_opt import BayesianOptimization\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def LGB_bayesian(\n",
    "    #learning_rate,\n",
    "    num_leaves, \n",
    "    bagging_fraction,\n",
    "    feature_fraction,\n",
    "    min_child_weight, \n",
    "    min_data_in_leaf,\n",
    "    max_depth,\n",
    "    reg_alpha,\n",
    "    reg_lambda\n",
    "     ):\n",
    "    \n",
    "    # LightGBM expects next three parameters need to be integer. \n",
    "    num_leaves = int(num_leaves)\n",
    "    min_data_in_leaf = int(min_data_in_leaf)\n",
    "    max_depth = int(max_depth)\n",
    "\n",
    "    assert type(num_leaves) == int\n",
    "    assert type(min_data_in_leaf) == int\n",
    "    assert type(max_depth) == int\n",
    "    \n",
    "\n",
    "    param = {\n",
    "              'num_leaves': num_leaves, \n",
    "              'min_data_in_leaf': min_data_in_leaf,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'feature_fraction' : feature_fraction,\n",
    "              #'learning_rate' : learning_rate,\n",
    "              'max_depth': max_depth,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'objective': 'binary',\n",
    "              'save_binary': True,\n",
    "              'seed': 1337,\n",
    "              'feature_fraction_seed': 1337,\n",
    "              'bagging_seed': 1337,\n",
    "              'drop_seed': 1337,\n",
    "              'data_random_seed': 1337,\n",
    "              'boosting_type': 'gbdt',\n",
    "              'verbose': 1,\n",
    "              'is_unbalance': False,\n",
    "              'boost_from_average': True,\n",
    "              'metric':'auc'}    \n",
    "    \n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    trn_data = lgb.Dataset(X_bay.iloc[bayesian_tr_idx], label=y_bay.iloc[bayesian_tr_idx])\n",
    "    val_data = lgb.Dataset(X_bay.iloc[bayesian_val_idx], label=y_bay.iloc[bayesian_val_idx])\n",
    "    \n",
    "#     trn_data= lgb.Dataset(X_train, label=y_train)\n",
    "#     val_data= lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=0,\n",
    "                    early_stopping_rounds = 50)\n",
    "    \n",
    "    score = roc_auc_score(y_bay.iloc[bayesian_val_idx], clf.predict(X_bay.iloc[bayesian_val_idx],\n",
    "                                                                num_iteration=clf.best_iteration))\n",
    "\n",
    "    return score\n",
    "\n",
    "# Bounded region of parameter space\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (31, 600), \n",
    "    'min_data_in_leaf': (20, 50),\n",
    "    'bagging_fraction' : (0.3, 0.9),\n",
    "    'feature_fraction' : (0.3, 0.9),\n",
    "    #'learning_rate': (0.01, 0.05),\n",
    "    'min_child_weight': (0.00001, 0.01),   \n",
    "    'reg_alpha': (0, 1), \n",
    "    'reg_lambda': (0, 2.5),\n",
    "    'max_depth':(22,28),\n",
    "    \n",
    "}\n",
    "#'boosting_type':'gbdt', 'class_weight':None, 'colsample_bytree':0.7,\n",
    "#         'importance_type':'split', 'learning_rate':0.05, 'max_depth':25,\n",
    "#         'min_child_samples':20, 'min_child_weight':0.00298, 'min_split_gain':0.0,\n",
    "#         'n_estimators':300, 'n_jobs':-1, 'num_leaves':256, 'silent':False, 'subsample':0.7,\n",
    "#           'reg_alpha':0.38999, 'reg_lambda':2.0,'subsample_for_bin':200000, 'subsample_freq':1,\n",
    "#         'objective': 'binary', \"bagging_seed\": 8, 'metric': 'auc', 'random_state': 47\n",
    "\n",
    "LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)\n",
    "print(LGB_BO.space.keys)\n",
    "\n",
    "init_points = 10\n",
    "n_iter = 15\n",
    "\n",
    "\n",
    "print('-' * 130)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9664258363017348"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6935644339425675,\n",
       " 'feature_fraction': 0.7725296816300017,\n",
       " 'max_depth': 46.39828323880483,\n",
       " 'min_child_weight': 0.006238160424832098,\n",
       " 'min_data_in_leaf': 25.49865573053208,\n",
       " 'num_leaves': 498.19326945045844,\n",
       " 'reg_alpha': 1.1017830721124613,\n",
       " 'reg_lambda': 1.363540425320737}"
      ]
     },
     "execution_count": 1650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.39946038667342054,\n",
       " 'feature_fraction': 0.8185681480509112,\n",
       " 'max_depth': 25.525277231258432,\n",
       " 'min_child_weight': 0.008816543279726339,\n",
       " 'min_data_in_leaf': 46.433793218816774,\n",
       " 'num_leaves': 683.6811337096823,\n",
       " 'reg_alpha': 0.00836513336998701,\n",
       " 'reg_lambda': 1.4314950016913357}"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_BO.max['params']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
